# ğŸ§  Neural Core - DocumentaciÃ³n del Proyecto

## ğŸ“‹ DescripciÃ³n General

**Neural Core** es un motor neuronal modular construido completamente en Python puro, diseÃ±ado para aprender y experimentar con redes neuronales desde cero, sin frameworks externos.

## ğŸ¯ Objetivos del Proyecto

- **Construir desde cero**: Implementar redes neuronales sin dependencias pesadas
- **Modularidad**: Cada componente es intercambiable y extensible
- **EducaciÃ³n**: CÃ³digo limpio y bien documentado para aprendizaje
- **ExperimentaciÃ³n**: Facilitar pruebas con diferentes arquitecturas y optimizadores

## ğŸ“ Estructura del Proyecto

```
neural_core/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                    # NÃºcleo neuronal
â”‚   â”‚   â”œâ”€â”€ neuron.py           # Microneuronas individuales
â”‚   â”‚   â”œâ”€â”€ layer.py            # Capas de neuronas
â”‚   â”‚   â”œâ”€â”€ network.py          # Red neuronal completa
â”‚   â”‚   â”œâ”€â”€ activations.py      # Funciones de activaciÃ³n
â”‚   â”‚   â”œâ”€â”€ losses.py           # Funciones de pÃ©rdida
â”‚   â”‚   â”œâ”€â”€ optimizers.py       # Optimizadores modulares
â”‚   â”‚   â””â”€â”€ latent.py           # Espacio latente (Fase 5)
â”‚   â”œâ”€â”€ engine/                 # Motor de entrenamiento
â”‚   â”‚   â”œâ”€â”€ trainer.py          # Entrenamiento supervisado
â”‚   â”‚   â””â”€â”€ rl_trainer.py       # Entrenamiento RL (Fase 5)
â”‚   â””â”€â”€ app.py                  # Punto de entrada
â”œâ”€â”€ tests/                      # Tests de validaciÃ³n
â”‚   â”œâ”€â”€ test_gradients.py       # Test de gradientes
â”‚   â”œâ”€â”€ test_stability.py       # Test de estabilidad
â”‚   â””â”€â”€ test_validation.py      # ValidaciÃ³n bÃ¡sica
â”œâ”€â”€ examples/                   # Ejemplos prÃ¡cticos
â”‚   â”œâ”€â”€ train_xor.py            # XOR con diferentes optimizadores
â”‚   â”œâ”€â”€ compare_optimizers.py   # ComparaciÃ³n de optimizadores
â”‚   â””â”€â”€ train_rl_curriculum.py  # Auto-curriculum (Fase 5)
â”œâ”€â”€ docs/                       # DocumentaciÃ³n
â”‚   â””â”€â”€ proyecto.md             # Este archivo
â”œâ”€â”€ pyproject.toml              # ConfiguraciÃ³n del proyecto
â””â”€â”€ run_tests.py               # Script de tests
```

## ğŸ§© Fases Completadas

### âœ… Fase 1 - Microneuronas y Activaciones
- **Neuronas individuales** con pesos, bias y funciones de activaciÃ³n
- **Funciones de activaciÃ³n**: sigmoid, relu, tanh, linear
- **Derivadas** para backpropagation

### âœ… Fase 2 - Capas y Red Neuronal
- **Capas de neuronas** con conectividad completa
- **Red multicapa** con propagaciÃ³n forward/backward
- **Estructura modular** [inputs, hidden, ..., outputs]

### âœ… Fase 3 - Backpropagation y Trainer
- **Backpropagation completa** desde cero
- **Trainer** para entrenamiento supervisado
- **ValidaciÃ³n de gradientes** y estabilidad

### âœ… Fase 4 - Optimizadores y Estabilidad
- **Optimizadores modulares**: SGD, Momentum, Adam, RMSprop
- **Tests de estabilidad** y validaciÃ³n de backprop
- **ComparaciÃ³n de optimizadores** en ejemplos

### âœ… Fase 5 - Variable Latente z y Proyector
- **Variable latente z** para planificaciÃ³n interna
- **LatentProjector** con proyecciÃ³n lineal + tanh
- **IntegraciÃ³n completa** en NeuralNetwork
- **Ejemplos de entrenamiento** con variable latente

### âœ… Fase 6 - Mini Framework Autograd
- **Motor autograd completo** con propagaciÃ³n automÃ¡tica
- **Nodo Value** con sobrecarga de operadores
- **Operaciones matemÃ¡ticas**: +, -, *, /, tanh, sigmoid, relu
- **API funcional**: linear, mse_loss
- **Entrenamiento XOR** usando autograd sin backprop manual
- **ComparaciÃ³n con backprop manual** validada

### âœ… Fase 7 - Macro-Neuronas Cognitivas
- **Celda de memoria diferenciable** con decaimiento temporal
- **Macro-neurona** que combina input + memoria + gating
- **Razonamiento secuencial** aprendido sin datasets
- **Sistema cognitivo** con contexto temporal
- **Demo de memoria** funcionando con autograd

## ğŸ§  Fase 7 - Macro-Neuronas Cognitivas (Sistema Razonador)

### ğŸ“ Nueva Estructura:
```
src/
â”œâ”€â”€ autograd/           # Motor diferenciable (Fase 6)
â”œâ”€â”€ autograd/
â”‚   â”œâ”€â”€ value.py          # Nodo escalar con autograd
â”‚   â”œâ”€â”€ ops.py            # Funciones matemÃ¡ticas
â”‚   â””â”€â”€ functional.py     # API estilo PyTorch
â”œâ”€â”€ core/
â”‚   â””â”€â”€ adapters.py       # Enlace con red existente (futuro)
```

### ğŸ”§ Componentes Implementados:

#### **Value - Nodo Base**
```python
from autograd.value import Value

# Crear valores con autograd
x = Value(2.0)
y = Value(3.0)
z = x * y + x.relu()  # Operaciones encadenadas
z.backward()  # PropagaciÃ³n automÃ¡tica
print(x.grad)  # Gradiente calculado automÃ¡ticamente
```

#### **Operaciones Disponibles**
- **AritmÃ©ticas**: +, -, *, /, **
- **Activaciones**: tanh, sigmoid, relu, leaky_relu
- **Funciones**: exp, log
- **Red**: linear, mse_loss

#### **Ejemplo de Uso**
```python
# Entrenamiento XOR con autograd
from autograd.value import Value
from autograd.functional import linear, mse_loss

# Crear red con autograd
layer1 = make_layer(2, 4)  # Pesos como Value
layer2 = make_layer(4, 1)

# Forward automÃ¡tico
y_pred = forward(x)
loss = mse_loss(y_pred, y)
loss.backward()  # Â¡Sin backprop manual!
```

### ğŸ¯ Resultados Fase 6:
- **âœ… Motor autograd funcional** sin dependencias
- **âœ… PropagaciÃ³n automÃ¡tica** de gradientes
- **âœ… API intuitiva** estilo PyTorch
- **âœ… Entrenamiento XOR** convergente
- **âœ… ValidaciÃ³n** contra backprop manual

### ğŸ“Š ComparaciÃ³n de Enfoques:
| Enfoque | Backprop | Complejidad | Flexibilidad |
|---------|----------|-------------|--------------|
| Manual  | Manual   | Alta        | Baja         |
| Autograd| AutomÃ¡tica| Baja        | Alta         |

### ğŸš€ PrÃ³ximos Pasos:
- **Fase 7**: Macro-neuronas cognitivas
- **Fase 8**: Memoria y atenciÃ³n
- **VectorizaciÃ³n**: OptimizaciÃ³n con NumPy (opcional)

## ğŸš€ Uso RÃ¡pido

### Ejemplo BÃ¡sico - XOR
```python
from core.network import NeuralNetwork
from core import losses
from core.optimizers import Adam
from engine.trainer import Trainer

# Dataset XOR
dataset = [
    ([0.0, 0.0], [0.0]),
    ([0.0, 1.0], [1.0]),
    ([1.0, 0.0], [1.0]),
    ([1.0, 1.0], [0.0]),
]

# Crear red
nn = NeuralNetwork([2, 4, 1], activation="sigmoid")

# Entrenar
trainer = Trainer(
    nn,
    loss_fn=losses.mse_loss,
    loss_grad_fn=losses.mse_grad,
    optimizer=Adam(lr=0.01),
    batch_size=1
)

trainer.train(dataset, epochs=2000, verbose=True)
```

### ComparaciÃ³n de Optimizadores
```python
from core.optimizers import SGD, SGDMomentum, Adam, RMSprop

# Probar diferentes optimizadores
optimizers = [
    ("SGD", SGD(lr=0.1)),
    ("Momentum", SGDMomentum(lr=0.05, momentum=0.9)),
    ("Adam", Adam(lr=0.01)),
    ("RMSprop", RMSprop(lr=0.01)),
]

for name, optimizer in optimizers:
    trainer = Trainer(nn, losses.mse_loss, losses.mse_grad, optimizer=optimizer)
    trainer.train(dataset, epochs=1000)
```

## ğŸ§ª EjecuciÃ³n de Tests

### Tests de ValidaciÃ³n
```bash
# Instalar dependencias
uv sync

# Ejecutar todos los tests
uv run python run_tests.py

# Tests individuales
uv run python -m pytest tests/test_stability.py -v
```

### Ejemplos
```bash
# XOR con Adam
uv run python examples/train_xor.py

# Comparar optimizadores
uv run python examples/compare_optimizers.py
```

## ğŸ“Š Arquitectura del Sistema

### Microneurona (Neuron)
```python
class Neuron:
    def __init__(self, n_inputs: int, activation: str = "sigmoid", optimizer: Optimizer = None):
        self.weights: List[float]  # Pesos sinÃ¡pticos
        self.bias: float           # Sesgo
        self.activation: Activation # FunciÃ³n de activaciÃ³n
        self.optimizer: Optimizer   # Optimizador configurable
    
    def forward(self, inputs: List[float]) -> float:
        # PropagaciÃ³n hacia adelante
        pass
    
    def apply_gradients(self, dweights: List[float], dbias: float) -> None:
        # ActualizaciÃ³n con optimizador
        pass
```

### Red Neuronal (NeuralNetwork)
```python
class NeuralNetwork:
    def __init__(self, layer_sizes: List[int], activation: str = "sigmoid"):
        # [n_inputs, n_hidden1, n_hidden2, ..., n_outputs]
        pass
    
    def forward(self, inputs: List[float], z: List[float] = None) -> List[float]:
        # Forward con soporte para variables latentes
        pass
    
    def train_step(self, inputs: List[float], targets: List[float], lr: float) -> float:
        # Un paso completo de entrenamiento
        pass
```

## ğŸ¯ CaracterÃ­sticas Implementadas

### âœ… Funcionalidad Base
- **Redes feedforward** multicapa
- **Backpropagation** completo
- **Funciones de activaciÃ³n** con derivadas
- **Funciones de pÃ©rdida** MSE y BCE
- **OptimizaciÃ³n** con mÃºltiples algoritmos

### âœ… ValidaciÃ³n
- **Tests de estabilidad** con XOR
- **VerificaciÃ³n de convergencia**
- **ValidaciÃ³n numÃ©rica**
- **Sin dependencias externas pesadas**

### âœ… Modularidad
- **Componentes intercambiables**
- **Optimizadores plug-and-play**
- **Funciones de activaciÃ³n extensibles**
- **Tests automatizados**

## ğŸš€ PrÃ³ximos Pasos - Fase 5

### ğŸ§  Espacio Latente y Auto-Curriculum
- **Variables latentes z** para representaciÃ³n interna
- **Auto-curriculum learning** con RL
- **GeneraciÃ³n de tareas** dinÃ¡mica
- **Mente interna** para planificaciÃ³n

### ğŸ“ˆ Escalabilidad
- **Batch processing** con numpy
- **ParalelizaciÃ³n** bÃ¡sica
- **Persistencia** de modelos
- **VisualizaciÃ³n** de entrenamiento

## ğŸ“‹ Requisitos

- **Python**: >= 3.12
- **Gestor de paquetes**: uv (recomendado)
- **Sistema operativo**: Linux/macOS/Windows

## ğŸ† Logros

- âœ… **0 dependencias pesadas** - Python puro
- âœ… **100% modular** - Cada componente es intercambiable
- âœ… **Tests completos** - ValidaciÃ³n exhaustiva
- âœ… **DocumentaciÃ³n clara** - CÃ³digo educativo
- âœ… **Ejemplos prÃ¡cticos** - XOR y comparaciones

## ğŸ“ PropÃ³sito Educativo

Este proyecto sirve como:
- **Plataforma de aprendizaje** para redes neuronales
- **Base para experimentaciÃ³n** con arquitecturas
- **DemostraciÃ³n** de backpropagation desde cero
- **Puente** hacia frameworks mÃ¡s complejos

---

**Estado actual**: âœ… **Fase 4B Completada** - Sistema validado y listo para Fase 5
