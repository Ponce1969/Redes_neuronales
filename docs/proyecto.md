# ğŸ§  Neural Core - DocumentaciÃ³n del Proyecto

## ğŸ“‹ DescripciÃ³n General

**Neural Core** es un motor neuronal modular construido completamente en Python puro, diseÃ±ado para aprender y experimentar con redes neuronales desde cero, sin frameworks externos.

## ğŸ¯ Objetivos del Proyecto

- **Construir desde cero**: Implementar redes neuronales sin dependencias pesadas
- **Modularidad**: Cada componente es intercambiable y extensible
- **EducaciÃ³n**: CÃ³digo limpio y bien documentado para aprendizaje
- **ExperimentaciÃ³n**: Facilitar pruebas con diferentes arquitecturas y optimizadores

## ğŸ“ Estructura del Proyecto

```
neural_core/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                 # Alias automÃ¡ticos (autograd/core/engine)
â”‚   â”œâ”€â”€ autograd/
â”‚   â”‚   â”œâ”€â”€ __init__.py             # Reexporta Value
â”‚   â”‚   â”œâ”€â”€ value.py                # Nodo autograd
â”‚   â”‚   â”œâ”€â”€ functional.py           # linear, mse_loss, etc.
â”‚   â”‚   â””â”€â”€ ops.py                  # Operaciones auxiliares
â”‚   â”œâ”€â”€ core/autograd_numpy/
â”‚   â”‚   â”œâ”€â”€ __init__.py             # Tensor + pÃ©rdidas vectorizadas (Fase 10)
â”‚   â”‚   â”œâ”€â”€ tensor.py               # Motor NumPy minimalista
â”‚   â”‚   â””â”€â”€ loss.py                 # MSE / BCE vectorizados
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py             # Componentes cognitivos
â”‚   â”‚   â”œâ”€â”€ memory_cell.py          # Celda de memoria diferenciable
â”‚   â”‚   â”œâ”€â”€ macro_neuron.py         # Macro-neurona con gating
â”‚   â”‚   â”œâ”€â”€ reasoning_unit.py       # Unidad de razonamiento
â”‚   â”‚   â”œâ”€â”€ cognitive_block.py      # Bloque cognitivo modular
â”‚   â”‚   â”œâ”€â”€ cognitive_graph.py      # Grafo de bloques cognitivos
â”‚   â”‚   â”œâ”€â”€ trm_block.py            # Tiny Recursive Model (Fase 10)
â”‚   â”‚   â”œâ”€â”€ trm_act_block.py        # TRM con ACT + deep supervision (Fase 11)
â”‚   â”‚   â”œâ”€â”€ cognitive_graph_trm.py  # Grafo TRM adaptativo (Fase 12)
â”‚   â”‚   â”œâ”€â”€ cognitive_graph_hybrid.py # Grafo hÃ­brido (Fase 13)
â”‚   â”‚   â””â”€â”€ projection_layer.py     # AutoAlign (Fase 14)
â”‚   â”œâ”€â”€ engine/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py              # Entrenamiento supervisado
â”‚   â”‚   â”œâ”€â”€ rl_trainer.py           # Entrenamiento RL
â”‚   â”‚   â”œâ”€â”€ dataset.py              # Utilidades de datasets
â”‚   â”‚   â””â”€â”€ predictor.py            # Predictores utilitarios
â”‚   â””â”€â”€ app.py                      # Punto de entrada CLI
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ cognitive_agent_demo.py     # Bloque cognitivo secuencial
â”‚   â”œâ”€â”€ cognitive_graph_demo.py     # Grafo cognitivo (Fase 9)
â”‚   â”œâ”€â”€ trm_demo.py                 # XOR con TRM vectorizado (Fase 10)
â”‚   â”œâ”€â”€ trm_act_demo.py             # TRM con halting adaptativo (Fase 11)
â”‚   â”œâ”€â”€ trm_cognitive_graph_demo.py # Grafo TRM recursivo (Fase 12)
â”‚   â”œâ”€â”€ hybrid_graph_demo.py        # Grafo hÃ­brido TRM + CognitiveBlock (Fase 13)
â”‚   â””â”€â”€ hybrid_graph_autoalign_demo.py # AutoAlign dinÃ¡mico (Fase 14)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_network.py
â”‚   â”œâ”€â”€ test_neuron.py
â”‚   â”œâ”€â”€ test_trainer.py
â”‚   â””â”€â”€ test_cognitive_graph_hybrid.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ proyecto.md                 # DocumentaciÃ³n general
â”œâ”€â”€ pyproject.toml                  # ConfiguraciÃ³n del proyecto
â””â”€â”€ README.md
```

## ğŸ§© Fases Completadas

### âœ… Fase 1 - Microneuronas y Activaciones
- **Neuronas individuales** con pesos, bias y funciones de activaciÃ³n
- **Funciones de activaciÃ³n**: sigmoid, relu, tanh, linear
- **Derivadas** para backpropagation

### âœ… Fase 2 - Capas y Red Neuronal
- **Capas de neuronas** con conectividad completa
- **Red multicapa** con propagaciÃ³n forward/backward
- **Estructura modular** [inputs, hidden, ..., outputs]

### âœ… Fase 3 - Backpropagation y Trainer
- **Backpropagation completa** desde cero
- **Trainer** para entrenamiento supervisado
- **ValidaciÃ³n de gradientes** y estabilidad

### âœ… Fase 4 - Optimizadores y Estabilidad
- **Optimizadores modulares**: SGD, Momentum, Adam, RMSprop
- **Tests de estabilidad** y validaciÃ³n de backprop
- **ComparaciÃ³n de optimizadores** en ejemplos

### âœ… Fase 5 - Variable Latente z y Proyector
- **Variable latente z** para planificaciÃ³n interna
- **LatentProjector** con proyecciÃ³n lineal + tanh
- **IntegraciÃ³n completa** en NeuralNetwork
- **Ejemplos de entrenamiento** con variable latente

### âœ… Fase 6 - Mini Framework Autograd
- **Motor autograd completo** con propagaciÃ³n automÃ¡tica
- **Nodo Value** con sobrecarga de operadores
- **Operaciones matemÃ¡ticas**: +, -, *, /, tanh, sigmoid, relu
- **API funcional**: linear, mse_loss
- **Entrenamiento XOR** usando autograd sin backprop manual
- **ComparaciÃ³n con backprop manual** validada

### âœ… Fase 7 - Macro-Neuronas Cognitivas
- **Celda de memoria diferenciable** con decaimiento temporal
- **Macro-neurona** que combina input + memoria + gating
- **Razonamiento secuencial** aprendido sin datasets
- **Sistema cognitivo** con contexto temporal
- **Demo de memoria** funcionando con autograd

### âœ… Fase 8 - CognitiveBlock (Arquitectura Cognitiva Modular)
- **ReasoningUnit**: Unidad que combina percepciÃ³n y memoria para inferencias
- **CognitiveBlock**: Bloque cognitivo completo con percepciÃ³n, memoria y razonamiento
- **Arquitectura modular**: Componentes interconectables para construir mentes artificiales
- **Demo de predicciÃ³n secuencial**: Aprende patrones temporales sin supervisiÃ³n
- **IntegraciÃ³n completa**: Todos los componentes usan el motor autograd

### âœ… Fase 9 - CognitiveGraph (Mente Modular Emergente)
- **CognitiveGraph**: Red de CognitiveBlock interconectados
- **ComunicaciÃ³n interbloques**: Feedforward, recurrente y reflexivo
- **Memoria compartida**: Estado global accesible por todos los bloques
- **Demo `cognitive_graph_demo.py`**: Mente artificial con percepciÃ³n â†’ razonamiento â†’ decisiÃ³n
- **Semilla determinista**: `random.seed(42)` para reproducibilidad
- **Alias automÃ¡ticos**: `src/__init__.py` expone `autograd`, `core` y `engine`

### âœ… Fase 10 - Motor Tensor Vectorizado + TRM Base
- **Tensor** NumPy (`core/autograd_numpy`) como reemplazo de `Value` para operaciones vectorizadas
- **Funciones de pÃ©rdida** MSE/BCE adaptadas al nuevo motor
- **TRMBlock** recursivo con estado latente z y detach entre pasos
- **Demo `examples/trm_demo.py`**: TRM aprende XOR con actualizaciÃ³n aproximada

### âœ… Fase 11 - Deep Supervision + Adaptive Computation Time
- **TRM_ACT_Block** con neurona de halting y cÃ¡lculo adaptativo de pasos
- **Deep supervision** en cada iteraciÃ³n con pÃ©rdidas parciales
- **Demo `examples/trm_act_demo.py`** validando razonamiento adaptativo en XOR

### âœ… Fase 12 - CognitiveGraph TRM
- **CognitiveGraphTRM** para orquestar mÃºltiples TRM_ACT conectados
- **Step numÃ©rico** y reset de estados para simulaciones recursivas
- **Demo `examples/trm_cognitive_graph_demo.py`**: pipeline percepciÃ³n â†’ razonamiento â†’ decisiÃ³n
- **Tests `tests/test_trm_cognitive_graph.py`** asegurando estabilidad y resets

### âœ… Fase 13 - CognitiveGraph Hybrid
- **CognitiveGraphHybrid** integra CognitiveBlock clÃ¡sicos y TRM_ACT adaptativos
- **Compatibilidad bidireccional**: convierte salidas entre Value â†” Tensor automÃ¡ticamente
- **Demo `examples/hybrid_graph_demo.py`** demostrando razonamiento mixto
- **Tests `tests/test_cognitive_graph_hybrid.py`** validando reset y estabilidad

### âœ… Fase 14 - AutoAlign Layers
- **ProjectionLayer** genera proyecciones lineales aprendibles entre bloques con dimensiones distintas
- **CognitiveGraphHybrid** ahora crea proyecciones on-the-fly al conectar nodos con tamaÃ±os incompatibles
- **Demo `examples/hybrid_graph_autoalign_demo.py`** muestra conexiones sensor â†’ memory â†’ reasoner â†’ decision con AutoAlign
- **Tests `tests/test_cognitive_graph_hybrid.py`** cubren estabilidad con AutoAlign activado

## ğŸ§  Estructura Completa del Proyecto

### ğŸ”§ Componentes Implementados:

#### **Value - Nodo Base**
```python
from autograd.value import Value

# Crear valores con autograd
x = Value(2.0)
y = Value(3.0)
z = x * y + x.relu()  # Operaciones encadenadas
z.backward()  # PropagaciÃ³n automÃ¡tica
print(x.grad)  # Gradiente calculado automÃ¡ticamente
```

#### **Operaciones Disponibles**
- **AritmÃ©ticas**: +, -, *, /, **
- **Activaciones**: tanh, sigmoid, relu, leaky_relu
- **Funciones**: exp, log
- **Red**: linear, mse_loss

#### **Ejemplo de Uso**
```python
# Entrenamiento XOR con autograd
from autograd.value import Value
from autograd.functional import linear, mse_loss

# Crear red con autograd
layer1 = make_layer(2, 4)  # Pesos como Value
layer2 = make_layer(4, 1)

# Forward automÃ¡tico
y_pred = forward(x)
loss = mse_loss(y_pred, y)
loss.backward()  # Â¡Sin backprop manual!
```

### ğŸ¯ Resultados Fase 6:
- **âœ… Motor autograd funcional** sin dependencias
- **âœ… PropagaciÃ³n automÃ¡tica** de gradientes
- **âœ… API intuitiva** estilo PyTorch
- **âœ… Entrenamiento XOR** convergente
- **âœ… ValidaciÃ³n** contra backprop manual

### ğŸ“Š ComparaciÃ³n de Enfoques:
| Enfoque | Backprop | Complejidad | Flexibilidad |
|---------|----------|-------------|--------------|
| Manual  | Manual   | Alta        | Baja         |
| Autograd| AutomÃ¡tica| Baja        | Alta         |

### ğŸš€ PrÃ³ximos Pasos:
- **Fase 10**: Sistemas cognitivos multi-agente
- **VectorizaciÃ³n**: OptimizaciÃ³n con NumPy (opcional)
- **Persistencia**: Guardado/carga de pesos
- **Exportar mÃ¡s alias**: Evaluar exposiciÃ³n plana de `io`, `examples`

## ğŸš€ Uso RÃ¡pido

### Ejemplo BÃ¡sico - XOR
```python
from core.network import NeuralNetwork
from core import losses
from core.optimizers import Adam
from engine.trainer import Trainer

# Dataset XOR
dataset = [
    ([0.0, 0.0], [0.0]),
    ([0.0, 1.0], [1.0]),
    ([1.0, 0.0], [1.0]),
    ([1.0, 1.0], [0.0]),
]

# Crear red
nn = NeuralNetwork([2, 4, 1], activation="sigmoid")

# Entrenar
trainer = Trainer(
    nn,
    loss_fn=losses.mse_loss,
    loss_grad_fn=losses.mse_grad,
    optimizer=Adam(lr=0.01),
    batch_size=1
)

trainer.train(dataset, epochs=2000, verbose=True)
```

### ComparaciÃ³n de Optimizadores
```python
from core.optimizers import SGD, SGDMomentum, Adam, RMSprop

# Probar diferentes optimizadores
optimizers = [
    ("SGD", SGD(lr=0.1)),
    ("Momentum", SGDMomentum(lr=0.05, momentum=0.9)),
    ("Adam", Adam(lr=0.01)),
    ("RMSprop", RMSprop(lr=0.01)),
]

for name, optimizer in optimizers:
    trainer = Trainer(nn, losses.mse_loss, losses.mse_grad, optimizer=optimizer)
    trainer.train(dataset, epochs=1000)
```

## ğŸ§ª EjecuciÃ³n de Tests

### Tests de ValidaciÃ³n
```bash
# Instalar dependencias
uv sync

# Ejecutar todos los tests
uv run python run_tests.py

# Tests individuales
uv run python -m pytest tests/test_stability.py -v
```

### Ejemplos
```bash
# XOR con Adam
uv run python examples/train_xor.py

# Comparar optimizadores
uv run python examples/compare_optimizers.py
```

## ğŸ“Š Arquitectura del Sistema

### Microneurona (Neuron)
```python
class Neuron:
    def __init__(self, n_inputs: int, activation: str = "sigmoid", optimizer: Optimizer = None):
        self.weights: List[float]  # Pesos sinÃ¡pticos
        self.bias: float           # Sesgo
        self.activation: Activation # FunciÃ³n de activaciÃ³n
        self.optimizer: Optimizer   # Optimizador configurable
    
    def forward(self, inputs: List[float]) -> float:
        # PropagaciÃ³n hacia adelante
        pass
    
    def apply_gradients(self, dweights: List[float], dbias: float) -> None:
        # ActualizaciÃ³n con optimizador
        pass
```

### Red Neuronal (NeuralNetwork)
```python
class NeuralNetwork:
    def __init__(self, layer_sizes: List[int], activation: str = "sigmoid"):
        # [n_inputs, n_hidden1, n_hidden2, ..., n_outputs]
        pass
    
    def forward(self, inputs: List[float], z: List[float] = None) -> List[float]:
        # Forward con soporte para variables latentes
        pass
    
    def train_step(self, inputs: List[float], targets: List[float], lr: float) -> float:
        # Un paso completo de entrenamiento
        pass
```

## ğŸ¯ CaracterÃ­sticas Implementadas

### âœ… Funcionalidad Base
- **Redes feedforward** multicapa
- **Backpropagation** completo
- **Funciones de activaciÃ³n** con derivadas
- **Funciones de pÃ©rdida** MSE y BCE
- **OptimizaciÃ³n** con mÃºltiples algoritmos

### âœ… ValidaciÃ³n
- **Tests de estabilidad** con XOR
- **VerificaciÃ³n de convergencia**
- **ValidaciÃ³n numÃ©rica**
- **Sin dependencias externas pesadas**

### âœ… Modularidad
- **Componentes intercambiables**
- **Optimizadores plug-and-play**
- **Funciones de activaciÃ³n extensibles**
- **Tests automatizados**

## ğŸš€ PrÃ³ximos Pasos - Fase 15

### ğŸ§  Entrenamiento Conjunto AutoAlign + CognitiveGraph
- **OptimizaciÃ³n compartida**: combinar gradientes de Value y Tensor en presencia de proyecciones
- **Persistencia** de pesos de ProjectionLayer y estados vectorizados
- **Interfaz comÃºn** para mezclar bloques clÃ¡sicos, TRM, memoria y nuevas capas

### ğŸ“ˆ Escalabilidad
- **Batch processing** con NumPy para TRM y grafo cognitivo
- **EstadÃ­sticas de halting** y visualizaciÃ³n de pasos de razonamiento
- **Persistencia** de modelos y replay de grafos cognitivos

## ğŸ“‹ Requisitos

- **Python**: >= 3.12
- **Gestor de paquetes**: uv (recomendado)
- **Sistema operativo**: Linux/macOS/Windows

## ğŸ† Logros

- âœ… **0 dependencias pesadas** - Python puro
- âœ… **100% modular** - Cada componente es intercambiable
- âœ… **Tests completos** - ValidaciÃ³n exhaustiva
- âœ… **DocumentaciÃ³n clara** - CÃ³digo educativo
- âœ… **Ejemplos prÃ¡cticos** - XOR y comparaciones

## ğŸ“ PropÃ³sito Educativo

Este proyecto sirve como:
- **Plataforma de aprendizaje** para redes neuronales
- **Base para experimentaciÃ³n** con arquitecturas
- **DemostraciÃ³n** de backpropagation desde cero
- **Puente** hacia frameworks mÃ¡s complejos

---

**Estado actual**: âœ… **Fase 14 Completada** - AutoAlign funcionando en CognitiveGraphHybrid
