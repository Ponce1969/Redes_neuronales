```markdown
# üßÆ Fase 34 - Cognitive Benchmark Suite

## üéØ Objetivo

Implementar un **sistema profesional de benchmarking cient√≠fico** que permita comparar configuraciones del Reasoner con:
- ‚úÖ Reproducibilidad total (seeds, provenance, git tracking)
- ‚úÖ An√°lisis estad√≠stico riguroso (t-tests, CI, effect size)
- ‚úÖ Multi-run aggregation para validez estad√≠stica
- ‚úÖ Baselines autom√°ticos para referencia objetiva
- ‚úÖ Reportes multi-formato (MD, HTML, LaTeX, CSV, JSON)
- ‚úÖ Dashboard interactivo
- ‚úÖ API REST completa

---

## üß† Concepto Base

El benchmarking cient√≠fico debe ser:

1. **Reproducible**: Mismo config + mismo seed = mismos resultados
2. **Estad√≠sticamente v√°lido**: M√∫ltiples runs, confidence intervals, p-values
3. **Comparable**: Baselines objetivos, m√©tricas est√°ndar
4. **Auditable**: Provenance completo (git, environment, timestamps)
5. **Publicable**: Reportes listos para papers cient√≠ficos

```
Workflow t√≠pico:
Config ‚Üí N runs ‚Üí Aggregated metrics ‚Üí Statistical comparison ‚Üí Report
```

---

## üìÅ Estructura de Archivos

```
src/core/benchmark/
‚îú‚îÄ‚îÄ __init__.py                    # Exportaciones del m√≥dulo
‚îú‚îÄ‚îÄ configurations.py              # BenchmarkConfig con reproducibilidad ‚≠ê
‚îú‚îÄ‚îÄ metrics.py                     # M√©tricas cient√≠ficas avanzadas ‚≠ê
‚îú‚îÄ‚îÄ provenance.py                  # Tracking de reproducibilidad ‚≠ê
‚îú‚îÄ‚îÄ baseline.py                    # Estrategias baseline ‚≠ê
‚îú‚îÄ‚îÄ comparator.py                  # An√°lisis estad√≠stico ‚≠ê
‚îú‚îÄ‚îÄ benchmark_suite.py             # Runner principal ‚≠ê
‚îî‚îÄ‚îÄ report_generator.py            # Reportes multi-formato ‚≠ê

src/api/routes/
‚îî‚îÄ‚îÄ benchmark.py                   # API REST ‚≠ê

dashboard/
‚îî‚îÄ‚îÄ dashboard_benchmark.py         # Dashboard Streamlit ‚≠ê

examples/
‚îú‚îÄ‚îÄ benchmark_demo.py              # Demo b√°sico ‚≠ê
‚îî‚îÄ‚îÄ benchmark_scientific.py        # Demo cient√≠fico completo ‚≠ê

tests/
‚îî‚îÄ‚îÄ test_benchmark.py              # Tests exhaustivos ‚≠ê

docs/
‚îî‚îÄ‚îÄ fase34_benchmark_suite.md      # Esta documentaci√≥n
```

---

## üß© Componentes Principales

### 1Ô∏è‚É£ **BenchmarkConfig (`configurations.py`)**

Configuraci√≥n completa y reproducible de un experimento.

#### Caracter√≠sticas:

- ‚úÖ **Hashing √∫nico** para identificaci√≥n
- ‚úÖ **Seed control** para reproducibilidad
- ‚úÖ **Versionado** de configs
- ‚úÖ **Validaci√≥n** de par√°metros
- ‚úÖ **Serializaci√≥n** JSON/YAML

#### Par√°metros Principales:

| Categor√≠a | Par√°metros | Descripci√≥n |
|-----------|------------|-------------|
| **Identidad** | name, description, version, tags | Metadata |
| **Reproducibilidad** | seed, deterministic | Control de randomness |
| **Reasoner** | reasoner_mode, n_hidden, n_blocks | Arquitectura |
| **Curriculum** | use_curriculum, curriculum_type | Aprendizaje progresivo |
| **Evolution** | evolution_strategy, mutation_scale | Estrategia de optimizaci√≥n |
| **Training** | n_runs, max_epochs_per_stage | Par√°metros de entrenamiento |

#### Ejemplo de Uso:

```python
from core.benchmark import BenchmarkConfig

config = BenchmarkConfig(
    name="my_experiment",
    description="Curriculum with top-k gates",
    seed=42,
    use_curriculum=True,
    reasoner_mode="topk",
    topk_value=2,
    n_runs=5,  # M√∫ltiples runs para stats
)

# Hash √∫nico
print(config.hash())  # "a3b5c7d9e1f2"

# Guardar
config.to_json("configs/my_experiment.json")
```

#### Configs Pre-Definidas:

```python
from core.benchmark import BENCHMARK_CONFIGS, list_configs

# Listar disponibles
print(list_configs())
# ['baseline_random', 'curriculum_softmax', 'curriculum_topk', ...]

# Obtener config
config = BENCHMARK_CONFIGS["curriculum_softmax"]
```

---

### 2Ô∏è‚É£ **BenchmarkMetrics (`metrics.py`)**

M√©tricas cient√≠ficas completas para evaluar performance.

#### M√©tricas Implementadas (15+):

**Performance**:
- `final_loss`: Loss al final del entrenamiento
- `best_loss`: Mejor loss alcanzado
- `final_accuracy`: Accuracy final
- `best_accuracy`: Mejor accuracy

**Convergencia**:
- `convergence_epoch`: Primera √©poca bajo threshold
- `time_to_threshold`: Tiempo hasta converger
- `converged`: Booleano de convergencia

**Estabilidad**:
- `loss_std`: Desviaci√≥n est√°ndar del loss
- `loss_variance`: Varianza
- `training_stability`: Score de estabilidad [0, 1]
- `loss_trend_slope`: Pendiente de mejora

**Gates Cognitivos**:
- `gate_diversity`: Uniformidad en uso de bloques
- `gate_entropy`: Entrop√≠a de Shannon
- `gate_consistency`: Consistencia temporal
- `gate_utilization`: % de bloques activos
- `dominant_gates`: Bloques m√°s usados

**Eficiencia**:
- `total_epochs`: Total de √©pocas
- `total_training_time`: Tiempo en segundos
- `epochs_per_second`: Velocidad

**Generalizaci√≥n**:
- `train_loss` / `test_loss`
- `generalization_gap`: test - train
- `overfitting_score`: Grado de overfitting

#### Agregaci√≥n de M√∫ltiples Runs:

```python
from core.benchmark import BenchmarkMetrics

metrics_list = [run1_metrics, run2_metrics, run3_metrics]

# Agregar con estad√≠sticas
aggregated = BenchmarkMetrics.aggregate(
    metrics_list,
    confidence_level=0.95
)

# Acceder a estad√≠sticas
mean_loss = aggregated.get_mean("final_loss")
std_loss = aggregated.get_std("final_loss")
ci_low, ci_high = aggregated.get_ci("final_loss")

# Formateo autom√°tico
formatted = aggregated.format_metric("final_loss", precision=4)
# "0.0234 ¬± 0.0012 [0.0220, 0.0248]"
```

---

### 3Ô∏è‚É£ **BenchmarkProvenance (`provenance.py`)**

Sistema de rastreo completo para reproducibilidad.

#### Informaci√≥n Capturada:

**Environment**:
- Python version
- NumPy version
- OS, platform, machine
- CPU count

**Git State**:
- Commit hash
- Branch name
- Is dirty (uncommitted changes)
- Remote URL

**Random State**:
- Seed usado
- NumPy random state (serializado)

**Config Completa**:
- Full config JSON

#### Ejemplo de Uso:

```python
from core.benchmark import BenchmarkProvenance, verify_reproducibility

# Capturar provenance
provenance = BenchmarkProvenance.capture(config)

print(provenance.summary())
# Run ID: 20250106_230145_123456
# Config: curriculum_softmax (hash: a3b5c7d9e1f2)
# Timestamp: 2025-01-06T23:01:45
# Python: 3.10.0
# NumPy: 1.24.0
# OS: Linux 5.15.0-91-generic
# Git: main@a3b5c7d9
# Reproducible: ‚úÖ

# Verificar si es reproducible en ambiente actual
check = verify_reproducibility(provenance)

if not check["can_reproduce"]:
    for warning in check["warnings"]:
        print(f"‚ö†Ô∏è  {warning}")
```

---

### 4Ô∏è‚É£ **Baseline Strategies (`baseline.py`)**

Estrategias de referencia para comparaci√≥n objetiva.

#### Baselines Disponibles:

| Baseline | Descripci√≥n | Uso |
|----------|-------------|-----|
| **random_uniform** | Gates aleatorios uniformes [0, 1] | Baseline m√°s b√°sico |
| **random_softmax** | Gates con softmax aleatorio | Similar a Reasoner sin aprendizaje |
| **equal** | Todos los gates iguales (1/N) | Activaci√≥n uniforme |
| **binary_random** | Gates binarios (0 o 1) | On/Off aleatorio |
| **topk_random** | Activa K bloques aleatorios | Top-K sin aprendizaje |
| **first_k** | Siempre los primeros K | Estrategia determin√≠stica |
| **gaussian** | Distribuci√≥n gaussiana | Preferencia por bloques centrales |

#### Ejemplo de Uso:

```python
from core.benchmark import BaselineReasoner, evaluate_baseline

# Crear baseline reasoner
baseline = BaselineReasoner(strategy="random_uniform", n_blocks=3)

# Generar gates
state = np.random.rand(10)
gates = baseline.predict(state)

# Evaluar baseline en task
loss, accuracy = evaluate_baseline(
    baseline_strategy="random_uniform",
    graph=graph,
    X=X_train,
    Y=Y_train
)
```

---

### 5Ô∏è‚É£ **BenchmarkComparator (`comparator.py`)**

An√°lisis estad√≠stico riguroso de resultados.

#### Caracter√≠sticas:

- ‚úÖ **T-tests** para comparaci√≥n pareada
- ‚úÖ **Confidence Intervals** (95% default)
- ‚úÖ **Effect Size** (Cohen's d)
- ‚úÖ **Bonferroni correction** para m√∫ltiples comparaciones
- ‚úÖ **Friedman test** para m√∫ltiples grupos
- ‚úÖ **Ranking** autom√°tico

#### Ejemplo de Uso:

```python
from core.benchmark import BenchmarkComparator

comparator = BenchmarkComparator(confidence_level=0.95)

# Comparar dos configs
comparison = comparator.compare_two(
    metrics_a=results_config_a,
    metrics_b=results_config_b,
    metric="final_loss",
    config_name_a="Curriculum",
    config_name_b="Baseline"
)

print(comparison.summary())
# Comparaci√≥n: Curriculum vs Baseline
# M√©trica: final_loss
#
# Config A: 0.0234 ¬± 0.0012 (median: 0.0230)
# Config B: 0.0456 ¬± 0.0023 (median: 0.0450)
#
# T-test: t=-12.345, p=0.0001 ‚úÖ
# Cohen's d: 0.856 (large)
#
# Winner: A üèÜ
# Improvement: 48.7%

# Rankear todas las configs
ranking = comparator.rank_configs(results_dict, metric="final_loss")

for name, mean, std, rank in ranking:
    print(f"{rank}. {name:25s} | {mean:.4f} ¬± {std:.4f}")
```

---

### 6Ô∏è‚É£ **BenchmarkSuite (`benchmark_suite.py`)**

Runner principal que orquesta todo el proceso.

#### Caracter√≠sticas:

- ‚úÖ **Reproducibilidad** autom√°tica (seeds)
- ‚úÖ **Multi-run** con agregaci√≥n
- ‚úÖ **Provenance** capture
- ‚úÖ **Auto-save** de resultados
- ‚úÖ **Logging** estructurado

#### Ejemplo de Uso:

**Single Benchmark**:

```python
from core.benchmark import BenchmarkSuite, BENCHMARK_CONFIGS

suite = BenchmarkSuite(
    output_dir="data/benchmarks/results",
    verbose=True
)

result = suite.run_single(
    config=BENCHMARK_CONFIGS["curriculum_softmax"],
    reasoner_manager=reasoner_manager,
    graph=graph,
    save_results=True
)

# Acceder a resultados
print(f"Final loss: {result.metrics.get_mean('final_loss'):.4f}")
print(f"Run ID: {result.provenance.run_id}")
```

**Comparison**:

```python
comparison_report = suite.run_comparison(
    configs=[
        BENCHMARK_CONFIGS["curriculum_softmax"],
        BENCHMARK_CONFIGS["curriculum_topk"],
        BENCHMARK_CONFIGS["baseline_random"],
    ],
    reasoner_manager=reasoner_manager,
    graph=graph,
    metric="final_loss"
)

# Ver ranking
for name, mean, std, rank in comparison_report.ranking:
    print(f"{rank}. {name} | {mean:.4f}")
```

---

### 7Ô∏è‚É£ **ReportGenerator (`report_generator.py`)**

Generador de reportes multi-formato.

#### Formatos Soportados:

1. **Markdown** - Legible, versionable con git
2. **HTML** - Interactivo con tablas
3. **LaTeX** - Para papers cient√≠ficos
4. **CSV** - An√°lisis en Excel/Pandas
5. **JSON** - Program√°tico

#### Ejemplo de Uso:

```python
from core.benchmark import ReportGenerator

generator = ReportGenerator()

generator.generate_all(
    report=comparison_report,
    output_dir="data/benchmarks/reports/exp001",
    formats=["markdown", "html", "latex", "csv", "json"]
)

# Genera:
# - report.md
# - report.html
# - report.tex
# - data.csv
# - data.json
```

---

## üåê API REST

### Endpoints Disponibles:

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| GET | `/benchmark/configs` | Lista configs disponibles |
| GET | `/benchmark/config/{name}` | Detalles de una config |
| POST | `/benchmark/run` | Ejecuta un benchmark |
| POST | `/benchmark/compare` | Ejecuta comparaci√≥n |
| GET | `/benchmark/status` | Estado actual |
| GET | `/benchmark/results` | Lista todos los resultados |
| GET | `/benchmark/result/{run_id}` | Detalle de un resultado |
| GET | `/benchmark/reports` | Lista reportes generados |
| DELETE | `/benchmark/results` | Limpia resultados |

### Ejemplos de Uso:

#### Listar Configs:

```bash
curl http://localhost:8000/benchmark/configs
```

#### Ejecutar Benchmark:

```bash
curl -X POST http://localhost:8000/benchmark/run \
  -H "Content-Type: application/json" \
  -d '{"config_name": "curriculum_softmax", "save_results": true}'
```

#### Ejecutar Comparaci√≥n:

```bash
curl -X POST http://localhost:8000/benchmark/compare \
  -H "Content-Type: application/json" \
  -d '{
    "config_names": ["curriculum_softmax", "curriculum_topk", "baseline_random"],
    "metric": "final_loss"
  }'
```

#### Ver Resultados:

```bash
curl http://localhost:8000/benchmark/results | jq
```

---

## üé® Dashboard Streamlit

### Lanzar Dashboard:

```bash
PYTHONPATH=src streamlit run dashboard/dashboard_benchmark.py
```

**Accede**: http://localhost:8504

### Caracter√≠sticas:

**3 Modos de Operaci√≥n**:

1. **üìä Ver Resultados**
   - Tabla de todos los resultados
   - Gr√°fico comparativo
   - Ver detalles completos de runs

2. **üöÄ Ejecutar Benchmark**
   - Selector de configs
   - Preview de par√°metros
   - Ejecuci√≥n con un click

3. **‚öñÔ∏è Comparar Configs**
   - Selecci√≥n m√∫ltiple de configs
   - Elecci√≥n de m√©trica
   - Reportes autom√°ticos

**Features**:
- Auto-refresh durante ejecuci√≥n
- M√©tricas en tiempo real
- Tabla interactiva
- Gr√°ficos Plotly
- Export de datos

---

## üöÄ Gu√≠a de Uso

### Opci√≥n 1: Demo B√°sico

```bash
PYTHONPATH=src python examples/benchmark_demo.py
```

### Opci√≥n 2: Demo Cient√≠fico

```bash
PYTHONPATH=src python examples/benchmark_scientific.py
```

### Opci√≥n 3: Con Servidor + Dashboard

```bash
# Terminal 1: Servidor
PYTHONPATH=src uv run uvicorn api.server:app --reload

# Terminal 2: Dashboard
PYTHONPATH=src streamlit run dashboard/dashboard_benchmark.py

# Terminal 3: Ejecutar benchmark
curl -X POST http://localhost:8000/benchmark/run \
  -d '{"config_name": "curriculum_fast"}'
```

### Opci√≥n 4: Program√°tico

```python
from core.benchmark import BenchmarkSuite, BENCHMARK_CONFIGS

suite = BenchmarkSuite()

result = suite.run_single(
    BENCHMARK_CONFIGS["curriculum_softmax"],
    reasoner_manager,
    graph
)

print(result.metrics.format_metric("final_loss"))
```

---

## üìä Casos de Uso

### Caso 1: Comparar Curriculum vs Baseline

```python
from core.benchmark import BenchmarkSuite, BENCHMARK_CONFIGS

suite = BenchmarkSuite()

report = suite.run_comparison(
    configs=[
        BENCHMARK_CONFIGS["curriculum_softmax"],
        BENCHMARK_CONFIGS["baseline_random"],
    ],
    reasoner_manager=reasoner_manager,
    graph=graph,
    metric="final_loss"
)

# ¬øCurriculum aprende mejor que random?
winner = report.ranking[0][0]
print(f"Winner: {winner}")
```

### Caso 2: Optimizar Hyperpar√°metros

```python
from core.benchmark import create_custom_config

# Probar diferentes mutation scales
configs = [
    create_custom_config(name=f"mutation_{scale}", mutation_scale=scale)
    for scale in [0.01, 0.03, 0.05, 0.1]
]

report = suite.run_comparison(configs, reasoner_manager, graph)

# Ver cu√°l funciona mejor
best_config = report.ranking[0][0]
```

### Caso 3: Validar Reproducibilidad

```python
# Run 1
result1 = suite.run_single(config, reasoner_manager, graph)

# Run 2 con mismo seed
result2 = suite.run_single(config, reasoner_manager, graph)

# ¬øResultados id√©nticos?
loss1 = result1.metrics.get_mean("final_loss")
loss2 = result2.metrics.get_mean("final_loss")

assert abs(loss1 - loss2) < 1e-6, "Not reproducible!"
```

---

## üéØ Mejores Pr√°cticas

### 1. Siempre usar N runs >= 3

```python
config = BenchmarkConfig(
    name="my_exp",
    n_runs=5,  # ‚úÖ M√≠nimo 3, ideal 5-10
)
```

### 2. Controlar seeds para reproducibilidad

```python
config = BenchmarkConfig(
    name="my_exp",
    seed=42,  # ‚úÖ Seed fijo
    deterministic=True,
)
```

### 3. Incluir baselines

```python
configs = [
    my_custom_config,
    BENCHMARK_CONFIGS["baseline_random"],  # ‚úÖ Baseline
]
```

### 4. Usar configs r√°pidas para testing

```python
# Testing
config_test = BENCHMARK_CONFIGS["curriculum_fast"]

# Production
config_prod = BENCHMARK_CONFIGS["curriculum_softmax"]
```

### 5. Guardar provenance

```python
result = suite.run_single(config, reasoner_manager, graph, save_results=True)

# ‚úÖ Provenance guardado autom√°ticamente
print(result.provenance.is_reproducible())
```

### 6. Generar reportes completos

```python
generator = ReportGenerator()
generator.generate_all(
    report,
    output_dir="reports/exp001",
    formats=["markdown", "html", "latex", "csv", "json"]
)
```

---

## üß™ Testing

### Ejecutar Tests:

```bash
# Todos los tests
pytest tests/test_benchmark.py -v

# Test espec√≠fico
pytest tests/test_benchmark.py::test_config_hash -v

# Con coverage
pytest tests/test_benchmark.py --cov=src/core/benchmark --cov-report=html
```

### Tests Implementados (30+):

- ‚úÖ BenchmarkConfig: creaci√≥n, hash, serializaci√≥n, validaci√≥n
- ‚úÖ BenchmarkMetrics: agregaci√≥n, CI, formateo
- ‚úÖ Provenance: captura, reproducibilidad
- ‚úÖ Baselines: generaci√≥n de gates, BaselineReasoner
- ‚úÖ Comparator: t-tests, ranking, effect size
- ‚úÖ Helper functions: stability, trend, consistency
- ‚úÖ Integraci√≥n: workflow completo

---

## üèÜ Beneficios del Sistema

### 1Ô∏è‚É£ **Validez Cient√≠fica**
- Multiple runs con agregaci√≥n estad√≠stica
- Confidence intervals al 95%
- P-values para significancia
- Effect sizes (Cohen's d)

### 2Ô∏è‚É£ **Reproducibilidad Total**
- Seeds controlados
- Provenance completo (git, env, timestamps)
- Verificaci√≥n autom√°tica
- State serialization

### 3Ô∏è‚É£ **Comparabilidad**
- Baselines objetivos
- M√©tricas est√°ndar
- Statistical tests
- Ranking autom√°tico

### 4Ô∏è‚É£ **Publicabilidad**
- Reportes LaTeX para papers
- HTML interactivo
- CSV para an√°lisis
- JSON program√°tico

### 5Ô∏è‚É£ **Automatizaci√≥n**
- API REST completa
- Dashboard interactivo
- CLI friendly
- CI/CD ready

---

## üìö Pr√≥ximos Pasos Sugeridos

Despu√©s de dominar la Fase 34:

### Fase 35: **Federated Reasoners**
- Conectar Orange Pi + Cloud
- Sincronizaci√≥n de experimentos
- Benchmark distribuido
- Agregaci√≥n de resultados federados

### Fase 36: **AutoML for Reasoners**
- Optimizaci√≥n autom√°tica de hyperpar√°metros
- Neural Architecture Search para Reasoner
- Meta-learning
- Transfer learning entre tasks

### Fase 37: **Benchmark Suite Extensions**
- More baselines (random forest, SVM, etc.)
- Cross-validation
- Bayesian optimization
- Wandb/MLflow integration

---

## üéì Comparaci√≥n con Fase 33

| Aspecto | Fase 33 (Curriculum) | Fase 34 (Benchmark Suite) |
|---------|---------------------|---------------------------|
| **Objetivo** | Entrenar progresivamente | Comparar cient√≠ficamente |
| **Output** | Reasoner entrenado | An√°lisis estad√≠stico |
| **Reproducibilidad** | Checkpoints | Provenance completo + seeds |
| **Comparaci√≥n** | Historial de etapas | T-tests, CI, effect size |
| **Baselines** | No | 8 baselines autom√°ticos |
| **Reportes** | Dashboard | 5 formatos (MD/HTML/LaTeX/CSV/JSON) |
| **Multi-run** | Single run | N runs con agregaci√≥n |
| **Validez cient√≠fica** | Observacional | Statistical rigor |

**Relaci√≥n**: Fase 33 entrena, Fase 34 valida y compara.

---

## üî• Conclusi√≥n

La Fase 34 implementa un **sistema de benchmarking de nivel cient√≠fico** que:

- ‚úÖ **Valida cient√≠ficamente** el aprendizaje curriculum (Fase 33)
- ‚úÖ **Compara** diferentes estrategias con rigor estad√≠stico
- ‚úÖ **Reproduce** experimentos exactamente
- ‚úÖ **Publica** resultados en formatos acad√©micos
- ‚úÖ **Automatiza** todo el pipeline de benchmarking

**Ahora puedes responder preguntas como**:
- ¬øCurriculum learning es significativamente mejor que random?
- ¬øQu√© configuraci√≥n de gates (softmax vs top-k) funciona mejor?
- ¬øLos resultados son reproducibles en otro ambiente?
- ¬øQu√© tan grande es el effect size?

**El sistema est√° listo para**:
- Papers cient√≠ficos (reportes LaTeX)
- An√°lisis de datos (CSV export)
- Automatizaci√≥n (API + CLI)
- Integraci√≥n continua (reproducibility checks)

---

**Autor**: Neural Core Team  
**Fase**: 34  
**Estado**: ‚úÖ Completo  
**Pr√≥ximo**: Fase 35 (Federated Reasoners) o Fase 36 (AutoML)
```
