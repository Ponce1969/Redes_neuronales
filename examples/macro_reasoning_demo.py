#!/usr/bin/env python3
"""
DemostraciÃ³n de razonamiento secuencial con Macro-neuronas Fase 7.
Aprende a recordar y predecir patrones temporales.
"""

import sys
import os
import random

# AÃ±adir el directorio src al path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from autograd.value import Value
from core.macro_neuron import MacroNeuron
from autograd.functional import mse_loss


def main():
    print("ðŸ§  Fase 7 - Macro-Neuronas Cognitivas")
    print("=" * 50)
    
    # Dataset: secuencia de bits -> salida = bit anterior
    # Esto simula un "eco temporal"
    sequences = [
        [0, 1, 0, 1, 1, 0],
        [1, 1, 0, 0, 1, 0],
        [0, 0, 1, 1, 0, 1],
        [1, 0, 0, 1, 0, 0],
    ]
    
    def get_target(sequence, t):
        """Retorna el bit anterior como target"""
        if t > 0:
            return [Value(float(sequence[t-1]))]
        else:
            return [Value(0.0)]  # Inicial
    
    # Crear macro-neurona
    macro = MacroNeuron(n_inputs=1, n_hidden=2, decay=0.8)
    lr = 0.05
    epochs = 1000
    
    print("ðŸŽ¯ Entrenando macro-neurona para razonamiento secuencial...")
    
    for epoch in range(epochs):
        total_loss = 0.0
        
        for seq in sequences:
            # Reset memoria para cada secuencia
            macro.reset()
            
            # Entrenar en cada paso temporal
            for t in range(len(seq)):
                x = [Value(float(seq[t]))]
                y_true = get_target(seq, t)
                
                # Forward pass
                y_pred = macro.forward(x)
                
                # Calcular pÃ©rdida
                loss = mse_loss(y_pred, [v.data for v in y_true])
                
                # Backward y actualizaciÃ³n
                loss.backward()
                
                # Actualizar pesos manualmente (sin optimizador por simplicidad)
                for w in macro.input_weights + macro.memory_weights + [macro.bias]:
                    w.data -= lr * w.grad
                    w.grad = 0.0
                
                total_loss += loss.data
        
        if epoch % 200 == 0:
            avg_loss = total_loss / (len(sequences) * len(sequences[0]))
            print(f"   Epoch {epoch:4d} | Loss={avg_loss:.6f}")
    
    print("\nðŸ“Š Prueba de razonamiento secuencial:")
    
    # Prueba con nueva secuencia
    test_seq = [0, 1, 0, 1, 1]
    macro.reset()
    
    print(f"   Secuencia de prueba: {test_seq}")
    print("   Entrada -> PredicciÃ³n (esperado)")
    print("   " + "-" * 30)
    
    for t in range(len(test_seq)):
        x = [Value(float(test_seq[t]))]
        y_pred = macro.forward(x)
        expected = get_target(test_seq, t)[0].data
        
        print(f"   {test_seq[t]} -> {y_pred[0].data:.4f} (esperado: {expected})")
    
    print("\nðŸŽ‰ Â¡Macro-neurona aprendiÃ³ razonamiento secuencial!")
    print("âœ… Fase 7 implementada exitosamente")
    print("âœ… Sistema cognitivo con memoria temporal activado")


if __name__ == "__main__":
    main()
