"""
Demo CientÃ­fico Avanzado de Benchmark Suite.

Demuestra reproducibilidad, anÃ¡lisis estadÃ­stico y generaciÃ³n de reportes.

Uso:
    PYTHONPATH=src python examples/benchmark_scientific.py
"""

import sys
from pathlib import Path
import numpy as np

# AÃ±adir src al path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from core.benchmark import (
    BenchmarkSuite,
    BenchmarkConfig,
    BenchmarkComparator,
    verify_reproducibility,
    create_custom_config,
    ReportGenerator,
)
from core.reasoning.reasoner_manager import ReasonerManager
from core.cognitive_graph_hybrid import CognitiveGraphHybrid
from core.cognitive_block import CognitiveBlock


def create_test_graph():
    """Crea grafo para tests."""
    graph = CognitiveGraphHybrid()
    
    sensor = CognitiveBlock(input_dim=8, hidden_dim=16, name="sensor")
    planner = CognitiveBlock(input_dim=16, hidden_dim=16, name="planner")
    decision = CognitiveBlock(input_dim=16, hidden_dim=8, name="decision")
    
    graph.add_block("sensor", sensor)
    graph.add_block("planner", planner)
    graph.add_block("decision", decision)
    
    graph.connect("sensor", "planner")
    graph.connect("planner", "decision")
    
    return graph


def main():
    """Ejecuta demo cientÃ­fico completo."""
    print("\n" + "="*70)
    print("ğŸ”¬ COGNITIVE BENCHMARK SUITE - DEMO CIENTÃFICO")
    print("="*70)
    print("\nEste demo demuestra:")
    print("  âœ… Reproducibilidad completa (seeds, provenance)")
    print("  âœ… AnÃ¡lisis estadÃ­stico (t-tests, CI, effect size)")
    print("  âœ… Multi-run aggregation")
    print("  âœ… Reportes multi-formato")
    print()
    
    # Setup
    graph = create_test_graph()
    reasoner_manager = ReasonerManager(n_inputs=24, n_hidden=48, n_blocks=3)
    suite = BenchmarkSuite()
    
    # ========================================================================
    # Experimento 1: Reproducibilidad
    # ========================================================================
    
    print("="*70)
    print("ğŸ§ª EXPERIMENTO 1: REPRODUCIBILIDAD")
    print("="*70)
    print()
    
    # Crear config con seed especÃ­fico
    config_repro = create_custom_config(
        name="reproducibility_test",
        description="Test de reproducibilidad con seed fijo",
        seed=42,
        use_curriculum=True,
        curriculum_type="fast",
        max_epochs_per_stage=20,
        n_runs=3,
    )
    
    print(f"Config creada: {config_repro.name}")
    print(f"Seed: {config_repro.seed}")
    print(f"Hash: {config_repro.hash()}")
    print()
    
    # Ejecutar benchmark
    print("Ejecutando benchmark...")
    result1 = suite.run_single(config_repro, reasoner_manager, graph, save_results=False)
    
    # Verificar provenance
    print("\nğŸ“‹ Provenance:")
    print(result1.provenance.summary())
    print()
    
    # Verificar reproducibilidad
    repro_check = verify_reproducibility(result1.provenance)
    print("ğŸ” VerificaciÃ³n de reproducibilidad:")
    print(f"   Can reproduce: {repro_check['can_reproduce']}")
    if repro_check['warnings']:
        for warning in repro_check['warnings']:
            print(f"   âš ï¸  {warning}")
    else:
        print("   âœ… Sin warnings")
    print()
    
    # ========================================================================
    # Experimento 2: ComparaciÃ³n EstadÃ­stica
    # ========================================================================
    
    print("="*70)
    print("ğŸ”¬ EXPERIMENTO 2: COMPARACIÃ“N ESTADÃSTICA")
    print("="*70)
    print()
    
    print("Configurando 3 estrategias:")
    print("  A. Curriculum + Softmax")
    print("  B. Curriculum + Top-K")
    print("  C. Baseline Random")
    print()
    
    configs = [
        create_custom_config(
            name="curriculum_softmax",
            use_curriculum=True,
            reasoner_mode="softmax",
            n_runs=5,
            max_epochs_per_stage=20,
            seed=42,
        ),
        create_custom_config(
            name="curriculum_topk",
            use_curriculum=True,
            reasoner_mode="topk",
            topk_value=2,
            n_runs=5,
            max_epochs_per_stage=20,
            seed=42,
        ),
        create_custom_config(
            name="baseline_random",
            use_curriculum=False,
            max_total_epochs=100,
            n_runs=5,
            seed=42,
        ),
    ]
    
    # Ejecutar comparaciÃ³n
    print("ğŸš€ Ejecutando comparaciÃ³n (puede tardar algunos minutos)...")
    print()
    
    comparison_report = suite.run_comparison(
        configs=configs,
        reasoner_manager=reasoner_manager,
        graph=graph,
        metric="final_loss",
    )
    
    print()
    print("="*70)
    print("ğŸ“Š RESULTADOS DE LA COMPARACIÃ“N")
    print("="*70)
    print()
    
    # Ranking
    print("ğŸ† RANKING:")
    for name, mean, std, rank in comparison_report.ranking:
        result = comparison_report.results[name]
        ci_low, ci_high = result.metrics.get_ci("final_loss")
        print(f"  {rank}. {name:25s} | {mean:.4f} Â± {std:.4f} | CI [{ci_low:.4f}, {ci_high:.4f}]")
    
    print()
    
    # AnÃ¡lisis estadÃ­stico detallado
    print("ğŸ”¬ ANÃLISIS ESTADÃSTICO:")
    print()
    
    comparator = BenchmarkComparator(confidence_level=0.95)
    
    for comp in comparison_report.comparisons:
        print(f"  {comp.config_a} vs {comp.config_b}:")
        print(f"    T-statistic: {comp.t_statistic:.3f}")
        print(f"    P-value: {comp.p_value:.4f} ({'âœ… significant' if comp.significant else 'âš ï¸  not significant'})")
        print(f"    Cohen's d: {comp.cohens_d:.3f} ({comp.effect_size_interpretation})")
        print(f"    Winner: {comp.winner} ğŸ† (improvement: {comp.improvement:.1%})")
        print()
    
    # Friedman test (si hay suficientes configs)
    metrics_dict = {
        name: result.all_runs
        for name, result in comparison_report.results.items()
    }
    
    friedman_result = comparator.friedman_test(metrics_dict, metric="final_loss")
    
    print("ğŸ“Š Friedman Test (mÃºltiples grupos):")
    print(f"   Statistic: {friedman_result['statistic']:.3f}")
    print(f"   P-value: {friedman_result['p_value']:.4f}")
    print(f"   Significant: {'âœ… Yes' if friedman_result['significant'] else 'âš ï¸  No'}")
    print(f"   Interpretation: {friedman_result['interpretation']}")
    print()
    
    # ========================================================================
    # Experimento 3: Reportes Multi-Formato
    # ========================================================================
    
    print("="*70)
    print("ğŸ“„ EXPERIMENTO 3: GENERACIÃ“N DE REPORTES")
    print("="*70)
    print()
    
    output_dir = Path("data/benchmarks/reports") / f"scientific_demo_{comparison_report.timestamp.strftime('%Y%m%d_%H%M%S')}"
    
    print(f"Generando reportes en: {output_dir}")
    print()
    
    generator = ReportGenerator()
    generator.generate_all(
        comparison_report,
        output_dir,
        formats=["markdown", "html", "latex", "csv", "json"],
    )
    
    print("âœ… Reportes generados:")
    print(f"   ğŸ“ Markdown: {output_dir}/report.md")
    print(f"   ğŸŒ HTML: {output_dir}/report.html")
    print(f"   ğŸ“Š LaTeX: {output_dir}/report.tex")
    print(f"   ğŸ“ˆ CSV: {output_dir}/data.csv")
    print(f"   ğŸ’¾ JSON: {output_dir}/data.json")
    print()
    
    # Mostrar preview del Markdown
    print("ğŸ“‹ Preview del reporte Markdown:")
    print("-" * 70)
    
    md_path = output_dir / "report.md"
    if md_path.exists():
        md_content = md_path.read_text()
        # Mostrar primeras 30 lÃ­neas
        lines = md_content.split("\n")[:30]
        print("\n".join(lines))
        if len(md_content.split("\n")) > 30:
            print("...")
            print(f"(+{len(md_content.split('\n')) - 30} lÃ­neas mÃ¡s)")
    
    print("-" * 70)
    print()
    
    # ========================================================================
    # Resumen Final
    # ========================================================================
    
    print("="*70)
    print("ğŸ‰ DEMO CIENTÃFICO COMPLETADO")
    print("="*70)
    print()
    print("ğŸ“Š Resumen de Experimentos:")
    print()
    print("  1ï¸âƒ£ Reproducibilidad:")
    print(f"      âœ… Provenance completo capturado")
    print(f"      âœ… Git commit: {result1.provenance.git_commit[:8] if result1.provenance.git_commit else 'N/A'}")
    print(f"      âœ… Reproducible: {result1.provenance.is_reproducible()}")
    print()
    print("  2ï¸âƒ£ ComparaciÃ³n EstadÃ­stica:")
    print(f"      âœ… {len(configs)} configuraciones evaluadas")
    print(f"      âœ… {configs[0].n_runs} runs por configuraciÃ³n")
    print(f"      âœ… Confidence level: 95%")
    print(f"      âœ… Winner: {comparison_report.ranking[0][0]} ğŸ†")
    print()
    print("  3ï¸âƒ£ Reportes:")
    print(f"      âœ… 5 formatos generados")
    print(f"      âœ… Listos para publicaciÃ³n cientÃ­fica")
    print()
    print("ğŸ“š PrÃ³ximos Pasos:")
    print("  1. Revisar reportes: cat", str(output_dir / "report.md"))
    print("  2. Abrir HTML:", str(output_dir / "report.html"))
    print("  3. Analizar datos: pandas.read_csv('{}')".format(output_dir / "data.csv"))
    print("  4. Dashboard: PYTHONPATH=src streamlit run dashboard/dashboard_benchmark.py")
    print()


if __name__ == "__main__":
    main()
